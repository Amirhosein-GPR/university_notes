#import "../assets/typst/tools/tool.typ"
#import "../assets/typst/tools/tool.typ": *

= جلسه چهاردهم
== Non-parameteric Models (ادامه)
=== Decision Tree
#tool.example()[
  فرض کنید داده هایی به صورت زیر داریم.
  بانک می خواهد بداند که کدام مشتری هایش احتمالاً خواهان دریافت وام هستند.
  Default borrower به این معنی است که آیا کسی به صورت پیش فرض از بانک وام می خواهد یا خیر.

  #tool.custom_figure(
    image("../images/ML/14_01.png"),
    caption: "داده های مشتری های یک بانک و درخت تصمیم مرتبط با آن.",
    inset: 1em,
  )

  از جدول بالا به صورت شهودی می توانیم به ساختار تصمیم گیری ای در سمت راست آن، به نام درخت تصمیم، برسیم.
]

#tool.double_section()[
  #tool.tip()[
    درخت تصمیم خوب، درختی است که به سرعت از ریشه به تصمیم برسد (با شرط های کمی برسد).
    یعنی درخت، پهن تر و کم عمق تر باشد.
  ]
][
  #tool.tip()[
    ویژگی خوب، ویژگی ای است که به وسیله آن زودتر به برگ برسیم و جامعه های خالص تری را ایجاد کند.
    #v(1.6em)
  ]
]

#tool.tip()[
  برای ساخت درخت تصمیم، این نکته را مد نظر قرار می دهیم که ویژگی ای که قرار است به عنوان ریشه درخت انتخاب شود، چقدر داده های ما را تمیز تقسیم می کند.
  تمیز به این معنا است که داده هایی که از تقسیم به دست می آیند، چقدر خالص اند.
  یعنی چقدر از نظر Label شبیه هم هستند.
]

#tool.example()[
  فرض کنید ویژگی هایی به نام $f_1$ و $f_2$ داریم که هر یک، دو مقدار Yes یا No را می توانند داشته باشند.
  در شکل زیر، داده ها را یک بار بر اساس $f_1$ و باری دیگر بر اساس $f_2$ تقسیم می کنیم:
  #tool.custom_figure(
    image("../images/ML/14_02.png"),
    caption: [تقسیم داده ها. یک بار بر اساس $f_1$ و یک بار بر اساس $f_2$.],
    inset: 1em,
    refrence: <image_14_02>,
  )
  با توجه به شکل بالا متوجه می شویم که وقتی بر اساس ویژگی $f_2$ تقسیم را انجام می دهیم، اکثریت داده های مثبت در سمت چپ و اکثریت داده های منفی در سمت راست قرار می گیرند.
  بنابراین تقسیم داده ها بر اساس ویژگی دو بهتر است.
  در نهایت، درخت تصمیم مناسبی را خواهیم ساخت، که به وسیله آن می توانیم نمونه های جدید را دسته بندی کنیم.
  اگر چه که ممکن است مانند شکل بالا، حتی با تقسیم داده ها بر اساس ویژگی $f_2$ همچنان مقداری خطا داشته باشیم.
]

==== الگوریتم های انتخاب ویژگی برای درخت تصمیم
#tool.list()[
  الگوریتم های انتخاب ویژگی برای ساخت درخت تصمیم شامل موارد زیر هستند:
  #grid(
    columns: (1fr, 1fr)
  )[
    + Hunt's Algorithm
    + CART
  ][
    3. ID3, C4.5, C5
    + SLIQ, SPRINT
  ]
]
===== الگوریتم Hunt
#tool.definition()[
  الگوریتمی است که به صورت تصادفی ویژگی ها را انتخاب می کند.
  درختی که در این روش ایجاد می شود، درختی درست اما پر هزینه است.
]

#tool.example()[
  عملکرد الگوریتم Hunt در مثال زیر به این شکل است:
  #tool.custom_figure(
    image("../images/ML/14_03.png"),
    caption: "نحوه کار الگوریتم Hunt",
    inset: 1em,
  )
  + در شروع کار (بخش a) که هنوز هیچ تقسیمی صورت نگرفته است، متوجه می شویم که در کل، ۳ داده مربوط به کلاس Yes و ۷ داده مربوط به کلاس No وجود دارد.

  + در ادامه (بخش b)، بر اساس ویژگی Home Owner، داده ها را به دو قسمت تقسیم می کنیم.
    مشاهده می شود که جمعیت سمت چپی حاصل از تقسیم، خالص است (داده های مربوط به تنها یک کلاس در آن قرار دارد).
    با جمعیتی که خالص است کاری نداریم و در ادامه سراغ جمعیت راست که ناخالص است، می رویم.

  + سپس (بخش c)، جمعیت سمت راست را بر اساس ویژگی Marital Status تقسیم می کنیم.
    این بار جمعیت سمت راست حاصل از تقسیم خالص است و در ادامه به سراغ جمعیت سمت چپ که ناخالص است می رویم.

  + در نهایت (بخش d)، جمعیت سمت چپ را بر اساس ویژگی Annual Income تقسیم می کنیم و چون این بار هر دو جمعیت حاصل از آن خالص هستند و هیچ جمعیت ناخالص دیگری وجود ندارد، الگوریتم به پایان می رسد.
]

#tool.tip()[
  در ساخت درخت های تصمیم، درخت های باینری را ترجیح می دهیم.
  چرا که درخت های باینری را می توان به سادگی با یک آرایه پیاده سازی کرد.
]

==== Binary کردن ویژگی های Continuous
#tool.example()[
  درآمد های افراد یک جامعه به ترتیب از چپ به راست در جدول زیر آمده اند:
  #tool.custom_figure(
    caption: "درآمد های افراد یک جامعه",
    kind: table,
    inset: 1em,
  )[
    #table(
      columns: 4,
      inset: 1em,
      stroke: black,
      align: center + horizon,
      "10", "12", "13", "14",
      "15", "18", "20", "30",
      "50", "100", "120",
    )
  ]

  برای این که بتوانیم بر اساس ویژگی درآمد که یک ویژگی غیر باینری است تقسیم باینری انجام دهیم، میانگین مینیمم و ماکزیمم آن را حساب می کنیم:
  $
    (10 + 120) / 2 = 65
  $
  در ادامه بر اساس این که درآمد ها بزرگ تر مساوی ۶۵ یا کوچک تر از ۶۵ هستند، درآمد ها را به دو قسمت تقسیم می کنیم.
  این روند را به همین ترتیب می توانیم ادامه دهیم تا درخت باینری مورد نظرمان ساخته شود.
  البته این که بر اساس میانگین مینیمم و ماکزیمم، تقسیم بندی را انجام دهیم هم همیشه درست نیست.
  راه حل جایگزین به صورت زیر است:

  در این مثال تک تک درآمد ها را به عنوان کلاسی جداگانه در نظر می گیریم و بر اساس آن ها تقسیم بندی دودویی را به شکل زیر انجام می دهیم:

  فرض کنید $x$ نماد داده های ورودی به درخت تصمیم است.
  بر اساس این که $x$ کدام یک از مقادیر درآمد ها را دارد، شرط های زیر را تولید می کنیم:
  $
    x <= 10 #h(1em) , #h(1em) x <= 12 #h(1em) , #h(1em) x <= 14 #h(1em) , #h(1em) dots #h(1em) , #h(1em) x <= 120
  $

  در نهایت به وسیله این شرط ها درخت تصمیم تشکیل می شود و به وسیله این درخت، نمونه های جدید طبقه بندی می شوند.
]

#tool.tip()[
  الگوریتم های انتخاب ویژگی مختلف، ویژگی هایی را انتخاب می کنند که خلوص بیشتری در تقسیم ایجاد می کنند.
]

==== روش های سنجش خلوص ویژگی ها
===== Misclassification Error
#tool.example()[
  این خطا برای @image_14_02 به صورت زیر محاسبه می شود:
  $
    "Error of" f_2 = 15 / 70 #h(1em) , #h(1em) "Error of" f_1 = 35 / 70
  $
  بنابراین $f_2$ برای تقسیم داده ها مناسب تر از $f_1$ است.
]

===== GINI Index (شکاف طبقاتی)
#tool.definition()[
  این شاخص به صورت زیر محاسبه می شود:
  $
    "GINI Index" = 1 - sum_(i=0)^(c-1) p_i (t)^2
  $
  که در آن $p_i (t)$ فراوانی کلاس $i$ ام در گره $t$ و $c$ تعداد کل کلاس ها است.
]

#tool.example()[
  فرض کنید افراد یک جامعه، یکی از حقوق های زیر را می گیرند:
  $
    "Salary" = {10, 20, 30, 40, 50}
  $
  نسبت تعداد افرادی که یکی از حقوق های بالا را می گیرند، به کل حقوق ها، به ترتیب به صورت زیر است:
  $
    "Ratio" = {0.3, 0.1, 0.1, 0.2, 0.3}
  $
  شکاف طبقاتی این جامعه به این صورت به دست می آید که ابتدا نسبت طبقات هر جامعه را به توان دو می رسانیم:
  $
    {0.09, 0.01, 0.01, 0.04, 0.09}
  $
  سپس مجموع شان را حساب می کنیم:
  $
    0.09 + 0.01 + 0.01 + 0.04 + 0.09 = 0.24
  $
  در نهایت مجموع به دست آمده را از ۱ کم می کنیم:
  $
    "شکاف طبقاتی" = 1 - 0.24 = 0.76
  $
]

#tool.tip()[
  هر چقدر جمعیت در طبقات بیشتری توزیع شده باشد، شکاف طبقاتی بیشتر است.
]

===== Entropy
#tool.definition()[
  شاخص تنوع یک جمعیت است و به صورت زیر محاسبه می شود:
  $
    "Entropy" = - sum_(i=0)^(c-1) p_i (t) log_2 p_i (t)
  $
  که در آن $p_i (t)$ فراوانی کلاس $i$ ام در گره $t$ و $c$ تعداد کل کلاس ها است.
  هر چقدر جمعیت خالص تر باشد، Entropy کمتر و هر چقدر جمعیت متنوع تر باشد، Entropy آن بیشتر است.
]

#tool.double_section()[
  #tool.tip()[
    هرچه GINI Index و Entropy به صفر نزدیک تر باشند، برای Classification بهتر اند.
  ]
][
  #tool.tip()[
    الگوریتم های #box[ID3]، C4.5 و C5 از Entropy و الگوریتم CART از GINI Index استفاده می کنند.
  ]
]

==== Gain
#tool.definition()[
  Gain مربوط به #box[Misclassification Error]، GINI Index و Entropy به این صورت محاسبه می شود که برای هر یک از این شاخص ها ابتدا شاخص جمعیت گره والد را محاسبه کرده و سپس همان شاخص را برای جمعیت های فرزندانش محاسبه می کنیم و میانگین آن ها را به دست می آوریم.
  در نهایت، میانگین به دست آمده را از شاخص جمعیت گره والد کم می کنیم.
  برای مثال Gain شاخص Entropy به شکل زیر محاسبه می شود:
  $
    "Gain"_"split" = "Entropy"(p) - sum_(i=1)^k n_i / n "Entropy"(i)
  $
  هر گره والدی که Gain بیشتری دهد، انتخاب می شود.
]

#tool.double_section()[
  #tool.list()[
    مزیت های Decision Tree:
    + ساده است.
    + Interpretable (قابل تفسیر) است.
  ]
][
  #tool.list()[
    عیب های Decision Tree:
    + Non-parameteric است.
    + هر تصمیم در درخت، بر اساس یک ویژگی گرفته می شود.
  ]
]


== Ensemble Models
#tool.definition()[
  مدل هایی هستند که از ترکیب بیش از یک مدل به وجود می آیند.
  Ensemble ها به این صورت کار می کنند که تعداد زیادی طبقه بند ساخته می شود و معمولاً از آن ها رأی گیری می کنیم.
  رأی گیری می تواند با وزن یا بدون وزن باشد.
  #tool.custom_figure(
    image("../images/ML/14_04.png"),
    caption: "رویکرد کلی Ensemble Learning",
    inset: 1em,
  )
]

=== Ad Hoc Ensembles
#tool.double_section()[
  #tool.definition()[
    روش های Ad Hoc (بزن و برو):
    به راه حلی می گویند که به طور خاص برای مسأله مشخصی کاربرد دارد و نمی توان به صورت عمومی از آن حتی برای حل مسائل مشابه استفاده کرد.
    Ad Hoc Ensembles به مدل های Ensemble ای می گویند که با تکنیک Ad Hoc ساخته شده اند.
    #v(1.6em)
  ]
][
  #tool.example()[
    درخت تصمیمی ساخته ایم. تست می کنیم که اگر در هر گره آن، یک SVM قرار دهیم عملکردش بهتر می شود یا خیر. اگر بهتر شد همین ساختار را حفظ می کنیم.
    به این ترتیب بدون این که فرمول عمومی دقیق و قابل انطباق با کلی از مسائل را داشته باشیم، می توانیم به این روش، عملکرد موضوع مورد نظر را بهبود بخشیم.
  ]
]

#tool.tip()[
  یکی از دلایل استفاده از Ensemble ها این است که به وسیله آن ها می توانیم با ترکیب تعدادی طبقه بند ساده، مسائل پیچیده را حل کنیم.
]

#tool.example()[
  داده های مربوط به دو کلاس زیر را به دو روش می توان از یکدیگر جدا کرد:
  + به وسیله یک سهمی
  + به وسیله سه خط ساده
  #tool.custom_figure(
    image("../images/ML/14_05.png", width: 60%),
    caption: "جدا سازی دو کلاس مختلف به کمک ۳ عدد خط در مقایسه با یک سهمی",
    inset: 1em,
  )
  به طبقه بند اول، طبقه بند Complex یا Strong می گویند.
  به طبقه بند دوم، که ترکیبی از ۳ طبقه بند ساده است، طبقه بند Weak می گویند.
]

#tool.example()[
  داده های مربوط به دو کلاس زیر را به دو روش می توان از یکدیگر جدا کرد:
  + به وسیله الگوریتم SVM
  + به وسیله الگوریتم Perceptron
  بهترین طبقه بند در شکل زیر همان خط وسط است که به وسیله SVM تشکیل می شود.
  الگوریتم Perceptron اما، بسته به نقطه شروعش، خط های متفاوتی تولید می کند.
  ۴ خط داریم که بدترین حالت های ممکن را ایجاد می کنند.
  برای این که خیال مان تا حد زیادی راحت شود که به این ۴ خط برخورد نمی کنیم، مثلاً ۴۰ بار الگوریتم Perceptron را اجرا می کنیم تا ۴۰ خط را تولید کند.
  با رأی گیری از این ۴۰ خط، به احتمال زیاد به دقت بالایی می رسیم.

  #tool.custom_figure(
    image("../images/ML/14_06.png", width: 60%),
    caption: "جدا سازی دو کلاس مختلف با SVM و Perceptron",
    inset: 1em,
  )
]

=== روش های ساخت Ensemble
==== Different Data
#tool.definition()[
  در این نوع Ensemble ها، Dataset به تعدادی قسمت تقسیم می شود و هر قسمت به یک Classifier برای آموزش آن داده می شود.
  در این جا دو رویکرد در تقسیم کردن Dataset وجود دارد:
  + تقسیم سطری (Record به Record)
  + تقسیم ستونی (Feature به Feature)
]
===== الگوریتم Bagging
#tool.definition()[
  در این الگوریتم، از Dataset تعدادی Subset با اعضای تصادفی انتخاب می شود و هر یک از مدل های تشکیل دهنده Ensemble با یکی از آن ها آموزش می بیند.
]

===== الگوریتم Random Forest
#tool.example()[
  نمونه ای با ویژگی های زیر داریم:
  $
    "Features" = {x_1, x_2, x_3, dots, x_10}
  $
  در ادامه هر یک از ویژگی ها را با احتمال انتخاب شدن ۵۰٪، برای آموزش مدل، انتخاب می کنیم.
  برای مثال این ویژگی ها در نهایت انتخاب شدند:
  $
    {x_1, x_5, x_10}
  $
  سپس با ویژگی های بالا درخت مان را می سازیم.
  بعد از ساخت درخت، دوباره از مجموعه ویژگی ها، با همان احتمال، ویژگی انتخاب می کنیم و درخت دومی را می سازیم.
  این کار را تا ساخت تعداد درخت مورد نظر تکرار می کنیم.
  مثلاً ۱۰۰۰ عدد.
  هنگام تست مدل، نمونه جدید با رأی گیری از درخت هایی که ساخته شدند، کلاس بندی می شود.
]

===== الگوریتم Boosting
#tool.example()[
  Dataset ای به شکل زیر داریم.
  احتمال انتخاب شدن هر یک از نمونه ها در ستون دوم نوشته شده است.
  به روش Random Forest تعدادی از نمونه ها را انتخاب می کنیم و با آن ها یک درخت تصمیم می سازیم.
  به کمک این درخت، کل Train Data را Label می زنیم و بررسی می کنیم که کدام داده ها را درست و کدام شان را اشتباه دسته بندی کرده است.
  این موارد در ستون Error مشخص شده اند.
  اگر طبقه بندی درست صورت گرفته باشد، مقدار ۰ و در غیر این صورت مقدار ۱ در این ستون قرار می گیرد.
  سپس دوباره تعدادی از نمونه ها را انتخاب می کنیم تا درخت بعدی را بسازیم.
  اما برخلاف الگوریتم Bagging، در این جا مجموعه نمونه هایی که از مجموعه اصلی نمونه ها می خواهیم انتخاب کنیم، لزوماً احتمال یکسانی برای انتخاب شدن ندارند.
  به این صورت که نمونه هایی که به اشتباه دسته بندی شدند (Error = 1)، شانس بیشتری برای انتخاب شدن برای ساخت درخت جدید خواهند داشت.
  به این صورت که شانس آن هایی که در مرحله قبل به اشتباه دسته بندی شدند را برای این مرحله مثلاً ۲ برابر می کنیم.
  اگر احتمال های جدید را با هم جمع کنیم، متوجه می شویم که مجموع آن ها ۱/۳ می شود که از ۱ بیشتر است.
  برای حل این مشکل آن ها را نرمال می کنیم.
  به این صورت که هر کدام از احتمال ها را تقسیم بر ۱/۳ می کنیم.
  به این ترتیب نمونه های جدید برای آموزش درخت انتخاب می شوند و این مراحل را برای درخت های جدید تکرار می کنیم (PBC = Probability of Being Chosen).
  #tool.custom_figure(
    caption: "نمونه های یک جمعیت و احتمال انتخاب شدن آن ها",
    kind: table,
    inset: 1em,
  )[
    #table(
      columns: 5,
      inset: 1em,
      stroke: black,
      align: center + horizon,
      "Instance", "PBC\n#1", "Error\n#1", "PBC\n#2", "Normalized PBC\n#2",
      "A", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "B", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "C", $display(1 / 10)$, $1$, $display(2 / 10)$, $display(2 / 13)$,
      "D", $display(1 / 10)$, $1$, $display(2 / 10)$, $display(2 / 13)$,
      "E", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "F", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "G", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "H", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
      "I", $display(1 / 10)$, $1$, $display(2 / 10)$, $display(2 / 13)$,
      "J", $display(1 / 10)$, $0$, $display(1 / 10)$, $display(1 / 13)$,
    )
  ]
]

#tool.tip()[
  از نظر این که احتمال انتخاب شدن مجدد نمونه ها چگونه تعیین می شود، الگوریتم های Boosting متفاوتی وجود دارند.
  از جمله #box[AdaBoost]، GentleBoost و XGBoost.
]

==== Different Architecture (Model)
#tool.definition()[
  این نوع Ensemble ها، از تعدادی Classifier مختلف تشکیل می شوند.
  برای مثال، Ensemble ای که از یک درخت تصمیم، شبکه عصبی و SVM، که هر یک به طور مستقل از روی کل داده های آموزشی ساخته شده اند، تشکیل شده است.
]
==== Different Training Parameters
#tool.definition()[
  این نوع Ensemble ها، از مدل های یکسان اما با پارامتر های متفاوت تشکیل می شوند.
]
