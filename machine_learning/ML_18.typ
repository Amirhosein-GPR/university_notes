#import "../assets/typst/tools/tool.typ"
#import "../assets/typst/tools/tool.typ": *

= جلسه هجدهم
== Reinforcement Learning
=== #emoji.mouse #emoji.cheese مسأله موش و پنیر #box(scale(emoji.cheese, x: -100%)) #box(scale(emoji.mouse, x: -100%))
#tool.example()[
  مسأله موش و پنیر (Rat in Maze)، نمونه مسأله ای در حوزه یادگیری تقویتی است.
  در این مسأله یک موش در یک صفحه شطرنجی قرار دارد و برای رسیدن به پنیر، باید با در نظر گرفتن موانع موجود، توالی ای از حرکت های بالا، راست، پایین و چپ را انجام دهد.
  موانع شامل خانه های خاکستری و نقاط خارج از صفحه شطرنجی می باشند.
  موش قبل از انجام حرکت بررسی می کند که حرکت در جهت مورد نظر باعث برخورد با مانع نشود.
  همچنین موش هیچ گاه در جهت خانه ای که از آن به خانه جدید وارد شده است، حرکت نمی کند.
  حرکت در هر یک از این ۴ جهت با یک احتمال که بیانگر احتمال رسیدن به پنیر است، انجام می شود.
  در این جا این مسأله را می خواهیم با روش یادگیری تقویتی حل کنیم.
  حل این مسأله به صورت زیر است:

  در ابتدا، موش اطلاعی از این که چه جهت حرکتی مناسب است، ندارد.
  بنابراین حرکت در هر یک از ۴ جهت با احتمال ۲۵٪ می تواند انجام شود و در نهایت موش در یکی از جهت ها حرکت می کند.
  با توجه به این موارد فرض کنید که موش انتخاب می کند که به سمت راست حرکت کند.
  اگر بعد از حرکتی که موش انجام می دهد به نقطه هدف برسد، حرکتی مناسب و در غیر این صورت حرکتی نامناسب انجام داده است.
  موش برای انجام حرکت مناسب، تشویق می شود.
  به این معنی که شانس حرکت مناسب برای انتخاب شدن در دفعات بعدی، افزایش و شانس ۳ حرکت دیگر، برای انتخاب شدن در دفعات بعدی، کاهش می یابد.
  شانس حرکاتی که منجر به برخورد با مانع می شوند صفر خواهد بود و مقدار شانس آن ها در حالتی که منجر به برخورد به مانع نمی شدند، بین بقیه حرکات تقسیم می شود.
  فرض کنید مطابق شکل زیر، موش در یک صفحه ۸ $times$ ۸ قرار دارد و می خواهد به پنیر برسد:

  #tool.custom_figure(
    image("../images/ML/18_01.png", width: 70%),
    caption: "صفحه شطرنجی مسأله موش و پنیر که در آن نقاط شروع و هدف و موانع مشخص شده اند.",
    inset: 1em,
  )

  با توجه به شکل بالا، فرض کنید موش حرکات زیر را انجام می دهد:
  $
    underbrace(S_1(6, 2), ("Top: 25 , Right: 25\nBottom: 25 , Left: 25")) arrow.long^"Right" underbrace(S_2(6, 3), ("Top: 50 , Right: 0\nBottom: 50 , Left: 0")) arrow.long^"Bottom" underbrace(S_3(7, 3), ("Top: 0 , Right: 34\nBottom: 33 , Left: 33")) arrow.curve.b^"Right"
    \
    underbrace(S_4(7, 4), ("Top: 0 , Right: 50\nBottom: 50 , Left: 0")) dots arrow.long^"Top" underbrace(S_100(3, 7), ("Top: 33 , Right: 34\nBottom: 0 , Left: 33")) arrow.long^"Top" S_101(2, 7)
  $
  در نهایت با انجام تعدادی حرکت تصادفی، موش به پنیر خواهد رسید.
  طبق حرکات بالا موش در گام $S_100$ به پایین خانه هدف می رسد و با انجام حرکت به سمت بالا، در گام $S_101$ وارد خانه هدف می شود.
  از آن جایی که به خانه هدف رسیدیم، باید احتمال حرکت به سمت بالا را در گام $S_100$ افزایش و احتمال حرکات دیگر را کاهش دهیم.
  یعنی:
  $
    underbrace(S_100(3, 7), ("Top: 100 , Right: 0\nBottom: 0 , Left: 0")) arrow.long^"Top" S_101(2, 7)
  $
  اما برای خانه های دیگری که در کنار خانه هدف نیستند چه باید کرد؟
  پاسخ این است که وقتی به خانه هدف رسیدیم، به خانه هدف مثلاً امتیاز ۱۰۰۰، و به خانه قبل آن (خانه ای که از آن وارد این خانه شدیم) امتیاز ۹۰۰ می دهیم.
  چرا که خانه قبل از خانه هدف، چسبیده به آن است و بر این اساس امتیازی بالا باید دریافت کند به گونه ای که این امتیاز، کمتر از امتیاز خانه هدف و بیشتر از امتیاز خانه های دیگری که همسایه خانه هدف نیستند، باشد.
  ارزش هر خانه طبق فرمول زیر حساب می شود (مثلاً با ضریب ۰/۹):
  $
    V = max(0.9 ("Top Cell Points", "Right Cell Points", \ "Bottom Cell Points", "Left Cell Points"))
  $
  یعنی از بین ۴ خانه همسایه، خانه همسایه ای انتخاب می شود که بیشترین امتیاز را بین آن ها دارد و سپس ۹۰٪ امتیاز آن به عنوان امتیاز خانه مورد نظر، ثبت خواهد شد.
  طبیعتاً موش حرکتی را انتخاب می کند که به خانه با امتیاز بیشتری برسد:
  $
    "Action" = "argmax"("Top", "Right", "Bottom", "Left")
  $
  به همین ترتیب به خانه های دیگر امتیاز می دهیم تا به خانه شروع برسیم:
  #tool.custom_figure(
    image("../images/ML/18_02.png", width: 70%),
    caption: "امتیاز مربوط به تعدادی از خانه ها در مسأله موش و پنیر.",
    inset: 1em,
  )

  همچنین موش می تواند جدولی از روی جدول امتیاز خانه ها، به صورت زیر بسازد و بر اساس آن حرکت های خود را انجام دهد:
  #tool.custom_figure(
    image("../images/ML/18_03.png", width: 70%),
    caption: "حرکت مربوط به تعدادی از خانه ها در مسأله موش و پنیر.",
    inset: 1em,
  )

  در این مرحله که به همگرایی و جدول Policy رسیدیم، به احتمال های مربوط به ۴ حرکتی که برای هر خانه حساب می کردیم، دیگر نیاز نمی شود و تنها کافی است همین جدول Policy را ذخیره نگه داریم و از آن استفاده کنیم.
]

#tool.list()[
  به ۴ مفهوم در مثال قبلی اشاره شد که عبارتند از:
  #set block(above: 0.8em)
  #grid(
    columns: (1fr, 1fr, 1fr, 1fr),
    align: center
  )[
    1. Environment
  ][
    2. Actor
  ][
    3. Action
  ][
    4. Result
  ]
]

#tool.tip()[
  روح و فلسفه Reinforcement Learning این است که یک Actor، با مجموعه ای از Action هایی که دارد، با Environment تعامل می کند و از Environment مجموعه ای از Result ها را دریافت می کند و بر اساس آن ها دانش و تصمیمات خود را بهبود می بخشد.
]

#tool.list()[
  یادگیری تقویتی در حل مسائل حوزه های گوناگون به کار می آید.
  مانند:
  #set block(above: 0.8em)
  #grid(
    columns: (1fr, 1fr),
    align: center
  )[
    1. Game-playing
    3. Credit-assignment
  ][
    2. Robot in a maze
    4. Learn a policy
  ]
]

=== Agent, Action, State and Reward
#tool.definition()[
  Agent موجودی است که درون محیط قرار دارد و یکی سری Action را انجام می دهد. گاهی وقت ها Action ای که انجام می شود باعث دریافت Reward (پاداش) و تغییر State محیط می شود.
]

#tool.example()[
  ماشین شانسی داریم که تعدادی دستگیره دارد و با انداختن یک سکه و حرکت دادن یکی از این دستگیره ها شروع به کار می کند.
  پس از آن ممکن است جایزه ای را دریافت کنیم یا این که هیچ جایزه ای دریافت نکنیم.
  اگر این دستگاه، ماشینی Deterministic باشد، پس از چندین بار تلاش، متوجه می شویم که کدام دستگیره نتیجه بهتری می دهند و ما را به پاداش مناسبی می رسانند.
  اما این دستگاه ها معمولاً Stochastic و شانسی هستند.
  یعنی با حرکت دادن دستگیره ای مشخص ممکن است در دفعه اول یک جایزه مشخصی بگیریم و در دفعه بعد، با حرکت دادن دقیقاً همان دستگیره جایزه دیگری بگیریم و در دفعه بعدتر کلاً هیچ جایزه ای نگیریم.
  ماشین شانس به صورت زیر تعریف می شود:
  $
    Q(a): "value of action" a
    space.quarter,
    "Reward is" r_a
    space.quarter,
    "Set" Q(a) = r_a
    \
    "Choose" a^* "if" Q(a^*) = "max"_a Q(a)
  $
  $
    "Rewards stochastic (keep an expected reward)": arrow.curve.b
    \
    Q_(t+1)(a) = Q_t (a) + eta [r_(t+1)(a) - Q_t (a)]
  $

  در فرمول بالا، $Q_t (a)$ برآوردی از میانگین ارزش عمل $a$ است.
  یعنی مثلاً فرض کنید عمل $a$ به کشیدن دستگیره شماره ۱ اشاره می کند.
  ما ۱۰۰۰ بار این دستگیره را حرکت دادیم و ۱۰۰۰ جایزه گرفتیم.
  میانگین این ۱۰۰۰ جایزه همان $Q_t (a)$ است که میانگین ارزش انجام عمل $a$ را نشان می دهد.
]
