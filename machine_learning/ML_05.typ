#import "../assets/typst/tools/tool.typ"
#import "../assets/typst/tools/tool.typ": *

= جلسه پنجم
#tool.simple_context()[
  اگر با هر الگوریتم دلخواه h را یادگیری بکنیم، بر اساس این که Hypothesis ما چه مقدار از C را پوشش می دهد، می توان کیفیت مدل را بررسی کرد.
]

#tool.reminder()[
  #tool.custom_figure(
    image("../images/ML/05_01.png"),
    caption: "یادآوری از جلسه قبل.",
    inset: 1em,
  )
]

#tool.reminder()[
  #text(dir: ltr)[
    ML Elements:
    + Examples
    + Model
    + Training Algorithm
  ]
]

== S, G and Version Space
#tool.definition()[
  در جلسات قبل یک مدل از نمونه های مثبت (S در شکل زیر) و یک مدل از نمونه های منفی (G در تصویر زیر) ساختیم که در شکل زیر آمده اند:
  #tool.custom_figure(
    image("../images/ML/05_02.png"),
    caption: [به مدل S، #box[Most specific hypothesis] و به مدل G، #box[Most general hypothesis] می گویند.],
    inset: 1em,
  )
  S کوچکترین Hypothesis سازگار با نمونه های آموزشی و G بزرگترین Hypothesis سازگار با نمونه های آموزشی می باشد.
  در شکل بالا هر مستطیلی که بین S و G باشد، جزو مجموعه جواب های ما است.

]

#tool.double_section()[
  #tool.definition()[
    Version Space: به فضایی که از S شروع و به G ختم می شود، که همان مجموعه جواب های بین S و G است، می گوییم #box[Version Space] می گویند.
  ]
][
  #tool.definition()[
    مسائل خوش ریخت: مسائلی که دانسته های مسأله برای تعیین جواب دقیق کافی است.
    #v(1.6em)
  ]
]

#tool.double_section()[
  #tool.definition()[
    مسائل بد ریخت: مسائلی که دانسته های مسأله برای تعیین جواب دقیق کافی نیست.
    در این مسائل، تعداد معادله ها از تعداد مجهول ها کمتر است.
    #v(1.6em)
  ]
][
  #tool.tip()[
    یادگیری (ماشین) یک مسأله بد ریخت (ill-posed problem) می باشد.
    مثلاً در شکل پیشین بی نهایت مستطیل به عنوان جواب بین S و G وجود دارد و مسأله آن یک مسأله ill-posed می باشد.
  ]
]

#tool.tip()[
  نمونه های بیشتر در فرآیند یادگیری مسأله را به سمت well-posed بودن می برد.
  #v(1.6em)
]

== انتخاب از درون Version Space
#tool.question()[
  در شکل پیشین از S تا G کدام مستطیل را اتنخاب کنیم؟
  (می خواهیم یک مورد از درون Version Space را انتخاب کنیم).
]

#tool.true_answer(pause: true)[
  1. می توان وسط آن را انتخاب کرد که این روش بر مبنای فرض Max Margin می باشد.
]

#tool.example()[
  با توجه به شکل زیر، فرض کنید نیاز های خانواده ها برای ماشین تغییر کند. این باعث می شود که تعریف ماشین خانواده کمی دچار تفاوت شود.
  مثلاً از این به بعد ماشین های خانواده نیاز است فضایی برای قرار دادن ماهواره برای اینترنت ماهواره ای داشته باشند.

  #tool.custom_figure(
    image("../images/ML/05_03.png"),
    caption: "قاعده Max Margin",
    inset: 1em,
  )

  بنابراین برای آن که مدل مان به تغییرات کوچک حساس نشود، مستطیلی را انتخاب می کنیم که با هر دو مستطیل S و G، مرز زیادی داشته باشد.
]

#tool.true_answer(continuation: true)[
  2. اگر مسأله طوری است که در آن مثلاً بیشترین ماشین ممکن را باید در گردونه ماشین های خانواده قرار دهیم، آن گاه باید طوری عمل کنیم که h به G نزدیکتر شود.

  3. اگر مسأله طوری است که در آن مثلاً ماشین خانواده یک چیز مشخص و معینی است، آن گاه باید طوری عمل کنیم که ماشین هایی که تفاوت کوچکی با مشخصات تعریف شده دارند، در دسته ماشین خانواده قرار نگیرند.
    در این حالت سعی می شود h به S نزدیک شود.
]

#tool.simple_context()[
  بعضی وقت ها برای انتخاب یک مورد از Version Space راهنمایی هایی همچون بالا را نداریم.
  در این صورت از قاعده کلی زیر (که می توان به دو صورت آن را پیاده سازی کرد) استفاده می شود.
]

#tool.definition(pause: true)[
  قاعده کلی انتخاب یک مورد از Version Space: معمولاً اگر از تعدادی Domain Expert که IID هستند (از هم مستقل هستند و با احتمال یکسان به طور تصادفی انتخاب می شوند)، سؤال بپرسیم، آنگاه رأی اکثریت به واقعیت نزدیک تر است.

  به دو صورت زیر این قاعده را با توجه به مثال شکل پیشین پیاده سازی می کنیم:

  می خواهیم مستطیلی را به صورت تصادفی بین S و G انتخاب کنیم. چگونه این کار را انجام می دهیم؟
  ۴ نقطه تصادفی بین S و G را به شکل زیر انتخاب می کنیم:
  #tool.custom_figure(
    image("../images/ML/05_04.png", width: 77%),
    caption: "مختصات ۴ نقطه درون ۴ بازه بالا که بین S و G هستند، به صورت تصادفی ساخته می شود.",
    inset: 1em,
  )
  از آن جایی که مستطیل مورد نظر بین مستطیل S و G می باشد، با نمونه های آموزشی مان سازگار است.
  دقت شود که در مثال بالا S و G از نمونه ها و h بین این دو به صورت تصادفی ساخته می شود.
]

#tool.question()[
  در روش بالا، چند مستطیل تصادفی باید بسازیم؟
]

#tool.true_answer()[
  $ N >= 33 $
  یعنی حداقل ۳۳ عدد.
  به دلیل قضیه حد مرکزی این عدد انتخاب می شود.
  حداقل تعداد نمونه های لازم در نمونه برداری از توزیع نرمال برای آن که توزیع به دست آمده از نمونه ها توزیع درستی باشد، ۳۳ (در اصل ۳۲ ولی در اینجا همان ۳۳ را در نظر بگیرید) عدد است.

  بنابراین از میان S و G (که همان Version Space مان است) N عدد مستطیل به صورت تصادفی انتخاب می کنیم.

  (برای فهم آنکه چرا N باید حداقل ۳۳ باشد به Normal Distribution Estimation و Central Limit Theorem مطالعه شود.)
]

#tool.definition(continuation: true)[
  پس از انتخاب N مستطیل به صورت توضیح داده شده، الگوریتم Voting را اجرا می کنیم.
  به این صورت که وقتی نمونه جدیدی به ما داده شود، Label آن Label ای است که اکثریت این N مستطیل اعلام می کنند.
]

#tool.tip()[
  قاعده Max Margin در کنار خوبی ای که دارد، دو عیب زیر را دارد:
  + باید بتوانیم برای هر Classifier، #box[Margin] را حساب کرد.
    این در حالی است که برای خیلی از Classifier ها به راحتی نمی توان Margin را حساب کرد.
  + این روش همیشه به ما کمک نمی کند.
    به این صورت که بعضی وقت ها یک Class مهم تر از یک Class دیگر است و باید Margin را کمی به این طرف و آن طرف هول داد.
]

== تقلید در یادگیری ماشین
#tool.tip()[
  در یادگیری ماشین با پشت صحنه پدیده ها کاری نداریم.
  بلکه با نمایش بیرونی آن ها کار داریم.
]

#tool.tip()[
  در یادگیری ماشین در اغلب اوقات برای ما مهم نیست که فرضیه ای که درباره موضوع مورد نظر می سازیم چقدر درست و چقدر غلط است.
  بلکه این مهم است که آن فرضیه چقدر مشاهدات ما را خوب توضیح می دهد.
]

#tool.example()[
  در زمان قدیم این فرضیه وجود داشت که زمین صاف است چرا که با استفاده از این فرضیه می توانستند شب و روز را مدل کنند، تقویم بسازند و جهت یابی کنند.
  بعداً مردم متوجه شدند که این فرضیه اشتباه است اما چیزی که باعث شد این فرضیه تا مدت ها پابرجا بماند این بود که برای مردم آن زمان کار می کرد و نیازشان را برطرف می کرد.
]

#tool.definition()[
  Digital Twin: برنامه ای کامپیوتری است که تلاش می کند رفتار یک سیستم واقعی را تقلید کند.

  #tool.custom_figure(
    image("../images/ML/05_05.png", width: 62%),
    caption: "Digital Twin سعی می کند رفتار Real System را تقلید کند.",
    inset: 1em,
  )

  دقت شود که درون Real System و Digital Twin لزوماً مانند یکدیگر نیست اما ورودی و خروجی یکسانی دارند.

  یکی از کاربرد های Digital Twin زمانی است که Real System گران قیمتی داریم و نمی توانیم هر کاری را با آن انجام دهیم.
]

== PAC Learning
#tool.definition()[
  #tool.custom_figure(
    image("../images/ML/05_06.png"),
    caption: "Probably Approximately Correct (PAC) Learning",
    inset: 1em,
  )
  فرض کنید C همان مفهومی است که واقعا وجود دارد و مساحت آن ۱ است.
  ما می خواهیم h را بسازیم.

  طبق شکل بالا خطای h شامل ۴ قسمت بالا، پایین، چپ و راستی است که بیرون از h و درون C قرار دارند.
  خطای h را $epsilon$ در نظر می گیریم.
  فرض کنیم مساحت ناحیه خطای شکل بالا $epsilon$ است و مساحت ۴ ناحیه با هم برابر است هر چند که اندازه شان متفاوت است.
  به این ترتیب مساحت هر یک از آن ها $epsilon \/ 4$ می باشد.

  احتمال این که یکی از ۴ نوار بالا به درون h بیافتد به صورت زیر است:
  $ 1 - epsilon / 4 $
  اگر N نمونه جدید پشت سر هم انتخاب کنیم، احتمال آن که همه این N نمونه از یک نوار خاص به درون h بیافتند، به صورت زیر است: $ (1 - epsilon / 4)^N arrow "Because samples are IID" $

  اگر N نمونه جدید پشت سر هم انتخاب کنیم، احتمال آن که همه این N نمونه از ۴ نوار به درون h بیافتاند، به صورت زیر است: $ 4 (1 - epsilon / 4)^N $

  احتمال بد شانسی را $delta$ در نظر می گیریم.
  اگر بخواهیم احتمال بد شانسی از یک $delta$ ای کمتر باشد، از رابطه زیر استفاده می کنیم:
  $ 4 (1 - epsilon / 4)^N <= delta $
  $delta$ کمیتی است که خودمان انتخاب می کنیم.
  مثلاً می خواهیم احتمال بد شانسی کمتر از ۰/۰۱ باشد.

  در ادامه به رابطه های آخر شکل بالا می رسیم که رابطه نهایی آن شکل، در زیر آورده شده است:

  $ N >= (4 / epsilon) log(4 / delta) $

  رابطه بالا تعیین می کند که برای یادگیری مستطیل سازگار با نمونه های آموزشی، باید حداقل چه تعداد نمونه داشته باشیم.

  به این تحلیل تئوریک، Probably Approximately Correct (PAC) Learning می گویند.

  $epsilon$: خطای نمونه

  $delta$: احتمال یادگیری مدل با استفاده از N نمونه آموزشی که خطای آن از $epsilon$ بیشتر باشد. $arrow.l$ احتمال بد شانسی
]

#tool.tip()[
  PAC Learning تکنیکی است برای تعیین حداقل تعداد نمونه های آموزشی برای آن که مدل خوب یاد گرفته شود و احتمال بد شانسی کم شود.
]

#tool.list()[
  در تکنیک PAC Learning دو مورد را مشخص می کنیم:
  + $epsilon$ (خطای قابل تحمل)
  + $delta$
  که در پاسخ، حداقل تعداد نمونه های آموزشی به دست می آید.
]

#tool.tip()[
  تحلیل هایی که در آن ها می خواهیم یک Boundry یا بازه را پیدا کنیم، اهمیت ندارند که دقیق باشند.
  این گونه تحلیل ها را تحلیل بد بینانه (Pessimistic) می گویند.
]

== مدل های یادگیری ماشین
#tool.double_section()[
  #tool.question()[
    چه مدلی بهتر است؟
    #v(3.6em)
  ]
][
  #tool.true_answer()[
    خبر بد:
    کسی نمی داند.

    خبر خوب:
    مجموعه ای از مدل های خوب برای مسائل عام شناسایی شدند.
  ]
]


#tool.tip()[
  مسأله ای که با یک مدل ساده تر حل می شود را با مدل پیچیده حل نمی کنند، چرا که:
  #tool.double_section()[
    + استفاده از آن ساده تر است.
    + Train کردن آن راحت تر است.
  ][
    3. توضیح دادن آن راحت تر است.
    + بهتر تعمیم می یابد.
  ]

  #tool.custom_figure(
    image("../images/ML/05_07.png", width: 86%),
    caption: "دلایل استفاده از مدل ساده تر",
    inset: 1em,
  )
]

#tool.list()[
  Classifier Models:
  + Basic Classifier: مدل هایی که با یک رابطه ریاضی یا یک ساختمان داده قابل توصیف اند.
    مانند:

    #text(dir: ltr)[Line, Polynomial, Decision Tree, Naive Bayes, Axis Aligned Rectangle, #sym.dots]
  + Ensemble Models (مدل های مرکب):
    مدل هایی هستند که چندین Basic Model درون خود دارند.
    مانند یک خط با یک مستطیل و یا مانند مثالی که مستطیل S و G را داشتیم و تعدادی مستطیل بین آن ها به طور تصادفی انتخاب کردیم و هنگام تولید Label از آن ها رأی گیری کردیم.
  + Neural Network & Deep Models: (فکر کنم یک جور هایی در هر دو گروه بالا می توانند قرار بگیرند)
]

=== Ensemble Models
#tool.example()[
  نمونه ای دیگر از یک Ensemble Model:

  می خواهیم دو گروه مثبت و منفی را از یکدیگر جدا کنیم که در شکل زیر آمده اند:
  #tool.custom_figure(
    image("../images/ML/05_08.png", width: 90%),
    caption: "جدا سازی دو گروه به کمک یک Ensemble Model",
    inset: 1em,
  )

  این کار در مراحل زیر انجام می گیرد:
  + جمع آوری نمونه ها #sym.arrow.l نمونه های مثبت و منفی
  + انتخاب مدل #sym.arrow.l خط
  + آموزش مدل #sym.arrow.l با کمک الگوریتم عمود منصف

  در این مثال الگوریتم یادگیری مورد استفاده الگوریتم عمود منصف است.
  این الگوریتم به این صورت کار می کند که مرکز جرم نمونه های مثبت و نمونه های منفی را پیدا کرده، سپس یک پاره خط که دو سر آن همان مرکز این دو گروه می باشد رسم می کنیم.
  عمود منصف این پاره خط، خطی است که این دو گروه را از یک دیگر جدا می کند.


  حال می خواهیم Ensemble ای از درخت های تصمیم بسازیم.
  این کار به این شکل انجام می شود که چندین بار از هر گروه تعدادی نمونه انتخاب کرده و مرکز آن نمونه ها را در هر گروه پیدا می کنیم.
  در ادامه طبق الگوریتم عمود منصف، خط جدا کننده این دو گروه پیدا می شود.
  در نهایت چندین خط جدا کننده به این روش تولید خواهند شد.
]
