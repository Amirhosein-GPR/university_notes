#import "../assets/typst/tools/tool.typ"
#import "../assets/typst/tools/tool.typ": *

= جلسه چهارم
== Smoothness
#tool.simple_context()[
  هدف در یادگیری ماشین (با کمی سهل گیری، هوش مصنوعی) این هست که دنیا را یک موجودیت پیوسته ببینیم، مدلش کنیم و این مدل را برای پیش بینی داده هایی که نداریم، استفاده کنیم.
  در دنیا موجودات مشابه هم، کنار هم هستند.
]

#tool.example()[
  Object زیر را داریم:
  $ "Object" cases("Attribute 1", "Attribute 2", "Attribute 3", dots.v, "Attribute N") $
]

#tool.double_section()[
  #tool.question()[
    دو Object ای که از یک جنس یا جمعیت هستند، انتظار داریم Attribute هاشون مشابه باشد یا نه؟
  ]
][
  #tool.true_answer()[
    انتظار داریم مشابه باشند.
    #v(3.2em)
  ]
]

#tool.double_section()[
  #tool.tip()[
    کل صحبتی که در یادگیری ماشین (با کمی سهل گیری، هوش مصنوعی) می شود، بر مبنای فرض Continuity یا Smoothness می باشد.
    #v(1.6em)
  ]
][
  #tool.example()[
    انتظار نداریم دیروز که هوا آفتابی بوده و امروز هم آفتابی هست فردا ناگهانی برفی بشود.
    حالت طبیعی این است که به تدریج هوا ابری شود و باد های سرد بوزد و به مرور هوا برفی شود.
  ]
]

#tool.reminder()[
  در مثال خرید ماشین خانواده، وقتی داده های مرتبطش را بر روی نمودار رسم کردیم، حدس زدیم که مستطیلی را می توان پیدا کرد که درون آن ماشین های مناسب خانواده قرار می گیرند. به این مستطیل، مدل می گویند. که به شکل زیر تعریف می شود:

  #align(center)[
    #text(dir: ltr)[
      Model\<Axis aligned rectangle\> = Model\<Left, Bottom, Width, Height\>
    ]
  ]

  #tool.custom_figure(
    image("../images/ML/04_01.png", width: 67%),
    caption: "مثال خرید ماشین مناسب خانواده در جلسه سوم",
    inset: 1em,
  )

  بعد از انتخاب مدل، باید پارامتر های آن آموزش داده شوند که به کمک ابزار یادگیری انجام می شود.

  #tool.custom_figure(
    image("../images/ML/04_02.png"),
    caption: "ابزار Learning که دو ورودی و یک خروجی به شکل بالا دارد.",
    inset: 1em,
  )

  در مدل آموزش داده شده، پارامتر های آن مقدار دهی شده اند.
]

== مفهوم مخفی و فرضیه سازگار
#tool.example()[
  فرض کنیم در مثال ماشین خانواده، ماشین های مناسب واقعا توسط یک مستطیل مشخص می شوند (مستطیل بزرگتر شکل زیر).

  #tool.custom_figure(
    image("../images/ML/04_03.png", width: 79%),
    caption: "مستطیل بزرگتر چیزی است که واقعا وجود دارد و مستطیل کوچک تر مدلی است که از یادگیری از روی مثال های داده شده به دست آمده است.",
    inset: 1em,
  )

  فرض کنیم شروع به تست کردن برنامه می کنیم و پاسخ آن این است که ماشین مناسب خانواده نیست، در صورتی که در واقع مناسب است.
]

#tool.double_section()[
  #tool.definition()[
    Latent concept: به مستطیلی که (در مثال بالا مستطیل است) واقعا وجود دارد و اغلب از آن خبر نداریم، می گویند و آن را با C نمایش می دهیم.
  ]
][
  #tool.definition()[
    Consistent hypothesis: چیزی که مدل یاد گرفت، که با نمونه های مثبت مان سازگار است، می گویند.
    #v(1.6em)
  ]
]

== بررسی یک الگو
#tool.example(pause: true)[
  فرض کنیم مساحت مستطیل کوچک $S_2$ و مساحت مستطیل بزرگ $S_1$ باشد.
  همچنین فرض کنیم:
  $ S_1 = 1 $
  در نتیجه:
  $ S_2 < S_1 $

  حال فرض کنیم یک نفر یک ماشین خانواده مناسب (نمونه مثبت) پیدا کرده و به ما می دهد (یعنی از کل C انتخاب کرده است).
]

#tool.double_section()[
  #tool.question()[
    احتمال این که از $S_2$ انتخاب کند چقدر است؟
  ]
][
  #tool.true_answer()[
    $S_2$
    #v(1.7em)
  ]
]

#tool.double_section()[
  #tool.question()[
    احتمال این که بیرون از $S_2$ انتخاب کند چقدر است؟
  ]
][
  #tool.true_answer()[
    $S_1 - S_2$
    #v(1.7em)
  ]
]

#tool.example(continuation: true, pause: true)[
  اگر اسم نمونه مورد نظر را $x$ بگذاریم آن گاه:
  $ p(x in S_2) = S_2 $
  $ p(x in.not S_2) = S_1 - S_2 $

  پس احتمالی وجود دارد که نمونه جدید به بهتر شدن فرضیه ما کمک می کند.
  این اتفاق زمانی می افتد که نمونه جدید در ناحیه درون $S_1$ و بیرون از $S_2$ باشد.

  احتمال اینکه همه N عدد نمونه ما درون $S_2$ باشند به صورت زیر است:
  $ (S_2)^N $

  به این ترتیب احتمال اینکه نمونه اول ما درون $S_2$ باشد، به صورت زیر است:
  $ S_2 $

  فرض کنیم $S_2$ نصف $S_1$ است.
  به این ترتیب احتمال این که اولین نمونه داخل $S_2$ باشد، ۵۰٪ درصد است.

  به نمونه هایی که می گیریم می گویند:
  // در این درس فرض ما این است که نمونه هایمان به صورت زیر است:
  #align(center)[
    #text(dir: ltr)[
      Independently and Identically Distributed (IID)
    ]
  ]
]

#tool.definition()[
  IID Sample: نمونه هایی که مستقل از هم اند و توزیع یکسانی دارند (احتمال یکسانی دارند).

  یعنی اگر مثلا ترتیب نمونه ها را عوض کنیم، چیزی عوض نمی شود.
]

#tool.double_section()[
  #tool.example()[
    از یک بشکه آب برداریم آبش کم می شود اما از یک چشمه آب برداریم گویی آبی کم نمیشه.
    در این جا چشمه IID است ولی بشکه IID نیست.
    #v(4.8em)
  ]
][
  #tool.example()[
    تعدادی توپ با رنگ های مختلف داریم.
    وقتی توپ ها را بر می داریم و رنگشان را نگاه می کنیم.
    اگر:
    + توپ ها را سر جایشان برگردانیم: نمونه ها IID هستند.
    + توپ را نگه داریم (IID): نمونه ها IID نیستند.
  ]
]
#tool.example(continuation: true)[
  فرض کنیم $S_2 = 0.5$ و همه نمونه ها IID هستند.
  احتمال این که نمونه اول داخل $S_2$ باشد و کمکی به یادگیری و بهبود Hypothesis ما نکند، برابر زیر است:
  $ (0.5) $

  نمونه دوم:
  $ (0.5)(0.5) $

  نمونه سوم:
  $ (0.5)(0.5)(0.5) $

  نمونه N ام:
  $ (0.5)^N $

  اگر بخواهیم احتمال بد شانسی ما کمتر از ۰/۰۱ باشد، (یعنی به احتمال ۰/۹۹ خوش شانس باشیم):
  $ P_"بد شانسی" = (0.5)^N <= 0.01 $
  $
    \
    (0.5)^N <= 0.01 arrow N log 0.5 <= log 0.01 arrow N >= (log 0.01) / (log 0.5) arrow N >= 6.7 tilde.eq 7
  $

  یعنی به ۷ نمونه نیاز داریم تا به احتمال ۰/۹۹ خوش شانس باشیم (مدل مان چیز جدید یاد بگیرد).

  در ادامه احتمال های بد شانسی مختلف به همراه تعداد نمونه لازم برای رسیدن به آن ها، آورده شده است:
  #tool.custom_figure(caption: [رابطه بین احتمال بد شانسی $S_2$ و تعداد نمونه ها], kind: table, inset: 1em)[
    #table(
      columns: 4,
      inset: 1em,
      stroke: black,
      align: center + horizon,
      fill: (x, y) => if calc.even(x) {
        luma(230)
      },
      [$S_2$ (بد شانسی)], "Samples\ncount", [$S_2$ (بد شانسی)], "Samples\ncount",
      "0.01", "1", "0.9", "43",
      "0.1", "2", "0.99", "458",
      "0.5", "7",
    )
  ]

  با دقت به جدول بالا به این نتیجه می رسیم که $S_2$ زمانی کوچکتر است که تعداد نمونه ها کم تر است.
]

#tool.tip()[
  هر چقدر در فرآیند آموزش از نمونه های جدید بیش تری استفاده می کنیم، از خوش شانسی به سمت بد شانسی می رویم.
  یعنی مثلاً اگر احتمال بد شانسی ما ۰/۹۹ است، با توجه به جدول قبلی، حداقل باید ۴۵۸ نمونه اضافه کنیم تا مدل مان بهتر شود.
  به عبارت دیگر یادگیری در ابتدا خیلی سریع است و به مرور کند تر می شود.

  #tool.custom_figure(
    image("../images/ML/04_04.png", width: 53%),
    caption: "اغلب نمودار های یادگیری به صورت بالا هستند. افت خطا در ابتدای آموزش که تعداد کمی نمونه دریافت می شود، زیاد است. اما از جایی به بعد نمونه های جدید خیلی کمکی نمی کنند.",
    inset: 1em,
  )
]

#tool.tip()[
  یادگیری ماشین را نمی توان بدون استفاده از نمونه انجام داد.
]

#tool.tip()[
  حجم نمونه آموزشی برای یادگیری مناسب، مهم است. نیاز است تعداد نمونه آموزشی مان از حداقل خاصی بیشتر باشد تا بتوان با کیفیت مد نظرمان یادگیری انجام شود.
]

== خطا و انواع آن
#tool.example()[
  فرض کنید Concept واقعی به صورت زیر است اما الگوریتم یادگیری به جای آن که از نمونه های مثبت یاد بگیرد از نمونه های منفی یاد بگیرد.
  #tool.custom_figure(
    image("../images/ML/04_05.png", width: 72%),
    caption: "در شکل بالا، مدل ما به صورت مستطیل چسبیده به نمونه های منفی تعریف می شود.",
    inset: 1em,
  )

  تفاوتی که مدل بالا با مدل قبلی دارد، به جز اینکه از نمونه های منفی برای آموزش استفاده می کند، این است که h ما به صورت کامل درون C قرار ندارد.

  ابتدا خطایی برای h تعریف می کنیم.
]

#tool.definition()[
  خطا: تعداد تشخیص های غلط برای نمونه های تستی مدل.

  انواع خطا ها شامل موارد زیر است:
  + خطای مطلق (Absolute error):
    این خطا در ریاضی کاربرد دارد و شامل مجموع همه خطا های موجود بر روی کل دامنه است.

  + خطای تجربی (Emprical error): تعداد خطا در مجموعه نمونه های کوچک و محدودی که داریم.
    شکل ریاضی این خطا به صورت زیر است:
    $ x, y: "Testing set" $
    $ E(h, x) = sum_(k = 1)^(abs(x)) 1(h(x^(<k>)) eq.not y^(<k>)) $
    در عبارت بالا $y$ برچسب تک تک $x$ ها است.
    مثلاً به صورت زیر:
    #tool.custom_figure(caption: "برچسب مرتبط با هر نمونه (هر نمونه فقط یک ویژگی x را دارد).", kind: table, inset: 1em)[
      #table(
        columns: 2,
        inset: 1em,
        stroke: black,
        align: center + horizon,
        [$x$], [$y$],
        [$x^(<1>)$], "Yes",
        [$x^(<2>)$], "No",
        [#sym.dots.v], [#sym.dots.v],
      )
    ]

    همچنین $h(x^(<k>)) eq.not y^(<k>)$ به معنای این است که Label ای که Hypothesis ما به $k$ امین نمونه آزمایش داده است، با Label واقعی مان متفاوت باشد.

  + خطای تجربی میانگین (Average emprical error):
    خطای تجربی تقسیم بر اندازه نمونه.
    این خطا برای مقایسه مدل ها به کار می رود چرا که مستقل از مجموعه Emprical شان است (بر خلاف Emprical error که قابل مقایسه نیست).
    شکل ریاضی این خطا به صورت زیر است:
    $ E_"Average" = 1 / (abs(x) + 1) sum_(k=1)^abs(x) 1(h(x^(<k>)) eq.not y^(<k>)) $
]

== انواع پاسخ های برنامه
#tool.definition()[
  انواع پاسخ هایی که برنامه ما می تواند تولید کند شامل موارد زیر است:
  + True Poisitive (TP)
  + True Negative (TN)
  + False Poisitive (FP)
  + False Negative / False Reject (FN / FR)
  که به صورت شهودی در شکل زیر آمده است:
  #tool.custom_figure(
    image("../images/ML/04_06.png", width: 55%),
    caption: "انواع پاسخ های برنامه ما",
    inset: 1em,
  )
]

== قاعده Maximum Entropy
#tool.tip()[
  تا اینجا دو مدل یاد گرفتیم.
  یک مدل بر پایه داده های مثبت به دست می آمد و مدل دیگر بر پایه داده های منفی.
  قاعده Maximum Entropy به ما می گوید مستطیلی (مدل ما در اینجا) را پیدا کنیم که دقیقا وسط دو مستطیل دیگر (دو مدل دیگر) باشد.
  #tool.custom_figure(
    image("../images/ML/04_07.png", width: 85%),
    caption: "مستطیل کوچک مدلی است که از نمونه داده های مثبت و مستطیل بزرگ مدلی است که از نمونه داده های منفی به دست آوردیم. مستطیلی که از قاعده Maximum Entropy به دست می آید، همان مستطیلی است که دقیقا وسط دو مستطیل دیگر قرار دارد.",
    inset: 1em,
  )

  این قاعده در همه جا ما را به نتایج خوبی می رساند.
]

#tool.example()[
  فرض کنید یک مسأله ای به شکل زیر داریم. چه چیزی نمونه های مثبت و منفی را از هم جدا می کند؟ بی نهایت خط.
  اما طبق قاعده Maximum Entropy مرز وسط به دست می آید.
  #tool.custom_figure(
    image("../images/ML/04_08.png", width: 60%),
    caption: "مرز بین دو گروه به وسیله خط وسطی مشخص شده است که از قاعده Maximum Entropy به دست می آید.",
    inset: 1em,
  )
]

#tool.tip()[
  یک Classifier محدود به یک Model نیست.
]

== VC-Dimension
#tool.double_section()[
  #tool.question()[
    فرض کنید کلاً یک نمونه مثبت داریم.
    با چه چیزی جداسازی را انجام می دهیم؟
  ]
][
  #tool.true_answer()[
    هیچ چیز.
    چون همه چیز مثبت است.
    #v(1.6em)
  ]
]

#tool.double_section()[
  #tool.question()[
    فرض کنید کلاً ۲ نمونه مثبت داریم.
    با چه چیزی جداسازی را انجام می دهیم؟
  ]
][
  #tool.true_answer()[
    هیچ چیز.
    چون همه چیز مثبت است.
    #v(1.6em)
  ]
]

#tool.double_section()[
  #tool.question()[
    فرض کنید کلاً ۲ نمونه داریم که یکی مثبت و دیگری منفی است.
    با چه چیزی جداسازی را انجام می دهیم؟
  ]
][
  #tool.true_answer()[
    با چیز های زیادی از جمله مستطیل، دایره، مثلث و ... اما ساده ترین شکل ممکن برای آن، خط است.
  ]
]

#tool.double_section()[
  #tool.tip()[
    اولین قاعده در یادگیری ماشین این است که مسأله ای که می خواهیم حل کنیم با ساده ترین مدل ممکن حل کنیم.
  ]
][
  #tool.question()[
    فرض کنید کلاً ۳ نمونه داریم که یکی مثبت و دو مورد دیگر منفی است (و یا بر عکس).
    با چه چیزی جداسازی را انجام می دهیم؟
  ]
]

#tool.true_answer()[
  با یک خط می توان جدا سازی را انجام داد.
]

#tool.double_section()[
  #tool.question()[
    فرض کنید کلاً ۴ نمونه داریم که غیر واقع بر یک خط راست هستند که دو مورد آن ها مثبت و دو مورد دیگر منفی هستند (و یا بر عکس).
    با چه چیزی جداسازی را انجام می دهیم؟
  ]
][
  #tool.true_answer()[
    نمی توان ۴ نقطه ای را پیدا کرد که یک خط بتواند همه جور برچسب زنی هایش را جدا کند.
    #v(3.2em)
  ]
]

#tool.double_section()[
  #tool.definition()[
    VC-Dimension یک Model: حداکثر تعداد نمونه هایی که مدل همه نوع برچسب گذاری آن ها را به درستی تفکیک می کند.
  ]
][
  #tool.example()[
    VC-Dimension یک خط برابر است با ۳.
    چرا که اگر بیش از ۳ نقطه داشته باشیم احتمال این که خط اشتباه کند وجود دارد.
  ]
]

#tool.double_section()[
  #tool.example()[
    VC-Dimension دو خط برابر است با ۵.
    چرا که اگر بیش از ۵ نقطه داشته باشیم احتمال این که دو خط اشتباه کنند وجود دارد.
  ]
][
  #tool.tip()[
    در یادگیری ماشین خطا اجتناب ناپذیر است.
    #v(3.2em)
  ]
]

#tool.double_section()[
  #tool.question()[
    مثال هایی وجود دارند که با وجود این که ۱۰۰۰ نمونه داریم به راحتی از یک خط استفاده می کنیم. چرا؟
    #v(1.6em)
  ]
][
  #tool.true_answer()[
    بنا به اصل پیوستگی انتظار چینش های عجیب و غریب نداریم و جاهایی که خط نمی تواند جدا کند حالت های بسیار خاص و بسیار نادر هستند.
  ]
]

#tool.tip()[
  تعداد نمونه های آموزشی مورد نیاز برای این که مدل خوب یاد بگیرد به لگاریتم معکوس خطا ربط دارد:
  $ N space alpha space log(1 / "error") $
  $N$ تعداد نمونه های آموزشی می باشد.
]
