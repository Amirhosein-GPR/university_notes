#import "../assets/typst/tools/tool.typ"
#import "../assets/typst/tools/tool.typ": *

= جلسه شانزدهم
== DBSCAN
#tool.definition()[
  الگوریتمی برای خوشه بندی است که در آن، نقاط فضای نمونه یکی از ۳ حالت زیر را دارند:
  + Internal یا Core: نمونه هایی که حداقل به اندازه Minimum Points همسایه دارند.
  + Border: نمونه هایی که کمتر از میزان Minimum Points همسایه دارند اما حداقل یک همسایه از نوع Core دارند.
  + Noise یا Anomaly: نمونه هایی که نه به تعداد Minimum Points همسایه دارند و نه حداقل یک همسایه از نوع Core دارند.

  در این الگوریتم، نقاطی که همسایه یکدیگرند باعث تشکیل خوشه ها می شوند.
  DBSCAN مانند K-Means، الگوریتمی Semi-automatic است.
  یعنی باید اطلاعاتی را از کاربر دریافت کند.
  این اطلاعات شامل ۲ مورد زیر هستند:
  + Neighbourhood Radius ($epsilon$): اگر فاصله دو نقطه از مقدار این پارامتر کمتر باشد، آن دو نقطه همسایه یکدیگر هستند.
  + Minimum Points: برای تشخیص نقاط Core به کار می رود.
]

#tool.example()[
  در شکل زیر نمونه هایی از نقاط #box[Core]، Border و Noise آورده شده اند:
  #tool.custom_figure(
    image("../images/ML/16_01.png", width: 50%),
    caption: [نمونه هایی از نقاط #box[Core]، Border و Noise],
    inset: 1em,
    refrence: <image_16_01>,
  )
]

#tool.tip()[
  الگوریتم DBSCAN:
  + فرض خاصی درباره شکل هندسی خوشه ها ندارد.
    بر خلاف K-Means که تنها خوشه های محدبی شکل را می توانست پیدا کند.
  + به اندازه خوشه ها حساس نیست.
  + الگوریتمی Single Pass است.
    یعنی یک بار نمونه ها را بررسی می کند و خوشه بندی را انجام می دهد.
  + در این الگوریتم از Silhouette Score می توان استفاده کرد.
]

#tool.tip()[
  اگر Silhouette Score نمونه ای منفی باشد، خوشه آن نمونه غیر محدب است.
]

#tool.example()[
  در @image_16_01 نقطه ای به عنوان نمونه انتخاب شده است.
  در این شکل، میانگین فاصله نقطه انتخاب شده با هم خوشه ای هایش، $a$ و میانگین فاصله اش از خوشه ای دیگر، $b$ است.
  بنابراین:
  $
    a > b arrow (b - a) / max(a, b) < 0
  $
  که یعنی خوشه ای که نقطه انتخابی ما را در بردارد، محدب نیست.
]

== Hierarchical Clustering
#tool.definition()[
  الگوریتمی برای خوشه بندی است که در ابتدا اطلاعاتی از کاربر نمی خواهد؛ بلکه هنگامی که به پایان برسد، کاربر می تواند تصمیم گیری کند که خروجی الگوریتم شامل چه تعداد خوشه است.
]

#tool.example(pause: true)[
  تعدادی نمونه به شکل زیر داریم.
  تعداد خوشه ها در ابتدا برابر با تعداد نمونه ها است.
  یعنی ۹ عدد.
  به صورت تکراری دو خوشه ای که نزدیک ترین به یکدیگرند، با هم ادغام می شوند.
  در نهایت تمامی نمونه ها در قالب یک خوشه در می آیند:
  #tool.custom_figure(
    image("../images/ML/16_02.png", width: 80%),
    caption: "خوشه بندی به کمک Hierarchical Clustering",
    inset: 1em,
    refrence: <image_16_02>,
  )
]

#tool.question()[
  مسأله ای که در خوشه بندی الگوریتم خوشه بندی سلسله مراتبی وجود دارد، این است که وقتی یک خوشه چند عضوی داریم و می خواهیم با ترکیب آن با یک نمونه جدید، خوشه ای جدید بسازیم، فاصله خوشه و نمونه جدید را چگونه بسنجیم؟
]

#tool.true_answer()[
  ۳ الگوریتم برای این کار وجود دارند:
  + الگوریتم Minimum Distance (حساس به Noise):
    فاصله دو به دو نقطه های دو خوشه را حساب کرده و کوچکترین فاصله از بین آن ها به عنوان فاصله دو خوشه انتخاب می شود.
  + الگوریتم Maximum Distance (حساس به Noise):
    فاصله دو به دو نقطه های دو خوشه را حساب کرده و بیشترین فاصله از بین آن ها به عنوان فاصله دو خوشه انتخاب می شود.
  + فاصله دو به دو نقطه های دو خوشه را حساب کرده و برای هر نقطه، Minimum فاصله آن با خوشه دیگر را حساب می کنیم.
    در نهایت از بین تمامی این Minimum ها Maximum گیری می کنیم.
    مقدار به دست آمده به عنوان فاصله دو خوشه تعیین می شود:
    $
      d(C_1, C_2) = max{min 1, min 2, dots, min 5}
    $
]

#tool.example(continuation: true)[
  در آخر، این خوشه بندی ها در قالب نمودار Dendrogram، به شکل زیر در می آیند:
  #tool.custom_figure(
    image("../images/ML/16_03.png", width: 80%),
    caption: [نمودار Dendrogram مربوط به @image_16_02],
    inset: 1em,
  )
  حال، ML Engineer تصمیم می گیرد که با توجه به نمودار بالا، داده ها را چگونه و در چند خوشه دسته بندی کند.
  مثلاً با رسم یک خط افقی فرضی از دومین انشعاب درخت، داده ها به ۳ خوشه تقسیم می بندی می شوند (مثل خوشه های انگور #emoji.face.lick).
]

#tool.tip()[
  ارتفاع یال ها در نمودار Dendrogram، با فاصله خوشه ها از یکدیگر، ارتباط دارد.
  یعنی هر چه ارتفاع آن ها بیشتر باشد، فاصله خوشه ها از یکدیگر بیشتر است.
  دقت شود که این موضوع در نمودار مثال بالا رعایت نشده است.
]

#tool.tip()[
  یکی از مواردی که می تواند یک دیدی برای قطع کردن درخت به ما بدهد، ارتفاع یال ها است.
  به طوری که اگر در قسمت هایی که یال های با ارتفاع بیشتری وجود دارند، درخت را قطع کنیم، معمولاً خوشه های خوبی به دست می آوریم.
  چرا که ارتفاع زیاد یال ها به این معنی است که خوشه ها فاصله زیادی از هم دارند.
]

== Noise, Outlier and Anomaly
#tool.comparision()[
  #box[Noise]، Outlier و Anomaly یک ویژگی مشابه دارند:
  + کم جمعیتی هستند که شباهت خیلی کمی با بقیه دارند.
  اما با یکدیگر تفاوت دارند:
  + Noise:
    داده ای که غلط است و با مقدار واقعی تفاوت دارد. (یه کمی غلطه #emoji.face.smile)
  + Outlier:
    داده ای که غلط است و با مقدار واقعی خیلی تفاوت دارد. (خیلی غلطه #emoji.face.smile)
  + Anomaly:
    داده ای که واقعاً وجود دارد اما با بقیه داده ها خیلی تفاوت دارد.
]

#tool.tip()[
  Anomaly خیلی وقت ها برای ما مفید است.
  مثلاً چک کردن ایمیل توسط یک نفر، روزی ۲ بار و هر بار ۵ دقیقه طول می کشد.
  حال اگر یک نفر باشد که روزی ۵۰ بار و هر بار به مدت ۱۰ ثانیه ایمیل را چک کند، شک می کنیم که ممکن است طرف مقابل یک ربات باشد و سرور ایمیل مورد حمله قرار گرفته باشد.
]

#tool.tip()[
  یکی از مهم ترین روش های تشخیص Anomaly، خوشه بندی است.
  خوشه هایی که معمولاً کوچک اند و فاصله زیادی با خوشه های دیگر دارند، می توانند Anomaly باشند.
]

== Anomaly Detection
=== Single Cluster (Density-based) Anomaly Detection
#tool.definition()[
  روشی برای تشخیص Anomaly است که در آن یک خوشه شامل داده های اصلی ایجاد می شود و بقیه داده ها به عنوان Noise در نظر گرفته می شوند.
  مانند شکل زیر:
  #tool.custom_figure(
    image("../images/ML/16_04.png", width: 35%),
    caption: "تک خوشه شامل داده های اصلی است و بقیه داده ها Noise هستند.",
    inset: 1em,
  )
]

#tool.tip()[
  هر ۳ الگوریتم خوشه بندی #box[K-Means]، DBSCAN و Hierarchical Clustering می توانند برای تشخیص Anomaly استفاده شوند.
]

=== Statistical Methods for Anomaly / Outlier Detection
#tool.definition()[
  + روش اول:
    فرض می کنیم داده هایی داریم که از یک توزیع نرمال آمده اند:
    $
      {1, 1, 2, 10, 20, 25, 20, 15, 35, 36, dots}
    $
    میانگین و انحراف معیار داده های بالا به صورت زیر است (شاید درست حساب نشده باشن. اگر درست نبود هم فرض کنید درسته. فقط یه مثاله #emoji.face.beam):
    $
      mu = 20 \
      sigma = 5
    $
    در این روش، که فرض کردیم داده های مان از یک توزیع نرمال آمده اند، بازه داده های عادی به صورت زیر حساب می شود و هر چه خارج از آن باشد Anomaly یا Outlier است (یادآوری: فرمول زیر همان فرمول بازه ۹۵٪ توزیع نرمال است):
    $
      mu plus.minus 2 sigma
    $
    که در مثال ما می شود:
    $
      20 plus.minus 2 times 5 arrow [10, 30]
    $
    در این روش اگر داده های مان زیاد باشد، داده هایی که درون بازه بالا قرار ندارند را حذف می کنیم.
    در غیر این صورت، مقداری پیش فرض به جای آن ها قرار می دهیم و یا یک مدل رگرسیون را با داده های عادی مان آموزش می دهیم و مقادیر به دست آمده از آن را جایگزین مقادیری که خارج از بازه اند، می کنیم و یا #sym.dots.

  + روش دوم:
    ابتدا چارک های داده ها را حساب می کنیم.
    در این روش، انواع چارک ها به صورت زیر تعریف می شوند.
    دقت شود که چارک های این روش با چارک هایی که در آمار وجود دارند، کمی متفاوت است (به علامت کف دقت شود):
    $
      \
      x floor 1 floor.r: q_0 #h(0.5em) , #h(0.5em) x floor N / 4 floor.r: q_1 #h(0.5em) , #h(0.5em) x floor N / 2 floor.r: q_2 #h(0.5em) , #h(0.5em) x floor (3 N) / 4 floor.r: q_3 #h(0.5em) #h(0.5em) , #h(0.5em) x floor N floor.r: q_4
    $
    برای محاسبه چارک های مورد نیاز، داده ها را مرتب می کنیم:
    $
      underbrace(1, display(q_0)), underbrace(1, display(q_1)), 2, 10, underbrace(15, display(q_2)), 20, underbrace(20, display(q_3)), 25, 35, 36, underbrace(50, display(q_4))
    $

    در ادامه، IQR یا همان فاصله چارکی را به صورت زیر حساب می کنیم:
    $
      "Inter Quartile Range (IQR)" = q_3 - q_1 = 20 - 1 = 19
    $

    در نهایت، بازه داده های معتبر به صورت زیر حساب می شود:
    $
      [q_1 - 1.5 "IQR", q_3 + 1.5 "IQR"]
    $
    که در مثال ما می شود:
    $
      [-27, 48]
    $
    که با توجه به داده هایمان، تنها عدد ۵۰، داده Outlier یا Anomaly است.
]
